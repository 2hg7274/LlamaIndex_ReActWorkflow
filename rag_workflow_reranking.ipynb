{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74442fbe",
   "metadata": {},
   "source": [
    "# RAG Workflow with Reranking  \n",
    "\n",
    "This notebook walks through setting up a `Workflow` to perform basic RAG with reranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4f249",
   "metadata": {},
   "source": [
    "## Designing the Workflow  \n",
    "\n",
    "RAG + Reranking consists of some clearly defined steps  \n",
    "\n",
    "1. Indexing data, creating an index  \n",
    "2. Using that index + a query to retrieve relevant text chunks  \n",
    "3. Rerank the text retrieved text chunks using the original query  \n",
    "4. Synthesizing a final response  \n",
    "\n",
    "With this in mind, we can create events and workflow steps to follow this process!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f795523",
   "metadata": {},
   "source": [
    "## The Workflow Events  \n",
    "\n",
    "To handle these steps, we need to define a few events:  \n",
    "1. An event to pass retrieved nodes to the reranker  \n",
    "2. An event to pass reranked nodes to the synthesizer  \n",
    "\n",
    "The other steps will use the built-in `StartEvent` and `StopEvent` events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02a923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class RetrieverEvent(Event):\n",
    "    \"\"\"Result of running retrieval\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    \"\"\"Result of running reranking on retrieved nodes\"\"\"\n",
    "\n",
    "    nodes: list[NodeWithScore]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebccdf1",
   "metadata": {},
   "source": [
    "## The Workflow Itself  \n",
    "\n",
    "With our events defined, we can construct our workflow and steps.  \n",
    "Note that the workflow automatically validates itself using type annotations, so the type annotations on our steps are very helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84054fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/2hg_llamaindex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:18<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"/media/user/datadisk2/LLM_models/Qwen2.5-14B-Instruct-1M\",\n",
    "    tokenizer_name=\"/media/user/datadisk2/LLM_models/Qwen2.5-14B-Instruct-1M\",\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "\n",
    "embed_model=HuggingFaceEmbedding(\n",
    "    model_name=\"/media/user/datadisk2/Embedding_models/all-MiniLM-L6-v2\",\n",
    "    device=\"cuda\",\n",
    "    target_devices=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12ea355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-07 16:37:21--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "arxiv.org (arxiv.org) 해석 중... 151.101.131.42, 151.101.3.42, 151.101.67.42, ...\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.131.42|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 301 Moved Permanently\n",
      "위치: http://arxiv.org/pdf/2307.09288 [따라감]\n",
      "--2025-07-07 16:37:21--  http://arxiv.org/pdf/2307.09288\n",
      "다음으로 연결 중: arxiv.org (arxiv.org)|151.101.131.42|:80... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 13661300 (13M) [application/pdf]\n",
      "저장 위치: `data/llama2.pdf'\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  10.5MB/s    / 1.2s     \n",
      "\n",
      "2025-07-07 16:37:23 (10.5 MB/s) - `data/llama2.pdf' 저장함 [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3beeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.workflow import (\n",
    "    Context,\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    step,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> StopEvent | None:\n",
    "        \"\"\"Entry point to ingest a document, triggered by a StartEvent with `dirname`.\"\"\"\n",
    "        dirname = ev.get(\"dirname\")\n",
    "        if not dirname:\n",
    "            return None\n",
    "\n",
    "        documents = SimpleDirectoryReader(dirname).load_data()\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents=documents,\n",
    "            embed_model=embed_model,\n",
    "        )\n",
    "        return StopEvent(result=index)\n",
    "\n",
    "    @step\n",
    "    async def retrieve(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> RetrieverEvent | None:\n",
    "        \"Entry point for RAG, triggered by a StartEvent with `query`.\"\n",
    "        query = ev.get(\"query\")\n",
    "        index = ev.get(\"index\")\n",
    "\n",
    "        if not query:\n",
    "            return None\n",
    "\n",
    "        print(f\"Query the database with: {query}\")\n",
    "\n",
    "        # store the query in the global context\n",
    "        await ctx.store.set(\"query\", query)\n",
    "\n",
    "        # get the index from the global context\n",
    "        if index is None:\n",
    "            print(\"Index is empty, load some documents before querying!\")\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever(similarity_top_k=2)\n",
    "        nodes = await retriever.aretrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} nodes.\")\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "    @step\n",
    "    async def rerank(self, ctx: Context, ev: RetrieverEvent) -> RerankEvent:\n",
    "        # Rerank the nodes\n",
    "        ranker = LLMRerank(\n",
    "            choice_batch_size=5, top_n=3, llm=llm\n",
    "        )\n",
    "        print(await ctx.store.get(\"query\", default=None), flush=True)\n",
    "        new_nodes = ranker.postprocess_nodes(\n",
    "            ev.nodes, query_str=await ctx.store.get(\"query\", default=None)\n",
    "        )\n",
    "        print(f\"Reranked nodes to {len(new_nodes)}\")\n",
    "        return RerankEvent(nodes=new_nodes)\n",
    "\n",
    "    @step\n",
    "    async def synthesize(self, ctx: Context, ev: RerankEvent) -> StopEvent:\n",
    "        \"\"\"Return a streaming response using reranked nodes.\"\"\"\n",
    "        summarizer = CompactAndRefine(\n",
    "            llm=llm, \n",
    "            streaming=True, \n",
    "            verbose=True,\n",
    "        )\n",
    "        query = await ctx.store.get(\"query\", default=None)\n",
    "\n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(result=response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a3515",
   "metadata": {},
   "source": [
    "And thats it! Let's explore the workflow we wrote a bit.  \n",
    "\n",
    "- We have two entry points (the steps that accept `StartEvent`)  \n",
    "- The steps themselves decide when they can run  \n",
    "- The workflow context is used to store the user query  \n",
    "- The nodes are passed around, and finally a streaming response is returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb6608",
   "metadata": {},
   "source": [
    "## Run the Workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b480519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = RAGWorkflow()\n",
    "\n",
    "# Ingest the documents\n",
    "index = await w.run(dirname=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f65d8651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query the database with: How was Llama2 trained?\n",
      "Retrieved 2 nodes.\n",
      "How was Llama2 trained?\n",
      "Reranked nodes to 2\n",
      " Llama 2 was trained using custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were performed on third-party cloud compute. Specifically, the pretraining utilized a cumulative 3.3M GPU hours of computation on A100-80GB hardware (with a TDP of 350-400W), resulting in estimated total emissions of 539 tCO2eq, all of which were offset by Meta's sustainability program. The pretraining data consisted of 2 trillion tokens from publicly available sources, with a cutoff of September 2022, while the fine-tuning data included publicly available instruction datasets and over one million new human-annotated examples."
     ]
    }
   ],
   "source": [
    "# Run a query\n",
    "result = await w.run(query=\"How was Llama2 trained?\", index=index)\n",
    "async for chunk in result.async_response_gen():\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d3f81",
   "metadata": {},
   "source": [
    "참고  \n",
    "- [RAG Reranking](https://docs.llamaindex.ai/en/stable/examples/workflow/rag/)  \n",
    "- [LLM Reranker Demonstration (Great Gatsby)](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LLMReranker-Gatsby/)  \n",
    "- [LLM Reranker Demonstration (2021 Lyft 10-k)](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LLMReranker-Lyft-10k/)  \n",
    "- [Structured LLM Reranker Demonstration (2021 Lyft 10-k)](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/Structured-LLMReranker-Lyft-10k/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2hg_llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
